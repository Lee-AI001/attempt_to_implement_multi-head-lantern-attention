{
    "project_settings": {
        "base_dir": "E:\\AI\\Model\\Transformer\\Storytelling",
        "project_name": "base_3.5M",
        "data_files": [
            "E:\\AI\\Model\\Transformer\\Storytelling\\data\\plot\\storytelling_pre.jsonl"
        ],
        "project_dir": "E:\\AI\\Model\\Transformer\\Storytelling\\Lantern.ai_v7_nova2\\storage\\base_3.5M"
    },
    "model_architecture": {
        "model_class": "StoryTellerTransformer",
        "vocab_size": 8000,
        "num_layers": 5,
        "d_model": 192,
        "nhead": 8,
        "dropout": 0.2,
        "embed_dropout": 0.05,
        "dim_feedforward": 768,
        "num_genres": 17,
        "d_c": 48,
        "d_prime_c": 48,
        "d_R_h": 16,
        "embedding_layer": "nn.Embedding(8000, 192)",
        "output_layer": "nn.Linear(192, 8000)",
        "decoder_layers": "5 layers of DecoderLayer(d_model=192, nhead=8, dim_feedforward=768, dropout=0.2, d_c=48, d_prime_c=48, d_R_h=16)",
        "normalization": "nn.LayerNorm(d_model)"
    },
    "chunking_tokenization": {
        "max_len": 512,
        "padding_value": 0,
        "sliding_step": 256,
        "max_chunks": 64
    },
    "training_hyperparameters": {
        "learning_rate": 0.0001,
        "weight_decay": 0.03,
        "batch_size": 64,
        "start_epoch": 0,
        "epochs": 48,
        "max_grad_norm": 1.0,
        "patience": 24,
        "use_optimizer": "adamw",
        "scheduler_type": "cosine",
        "warmup_epochs": 4,
        "label_smoothing": 0.1,
        "grad_accum_steps": 4,
        "test_size": 0,
        "rouge_sample_size": 10,
        "mixed_precision": "fp16",
        "plateau_factor": 0.5,
        "plateau_patience": 5,
        "use_optuna": false,
        "optuna_n_trials": 10
    },
    "generation_hyperparameters": {
        "split_ratio": 0.98,
        "max_gen_len": 256,
        "temperature": 1.2,
        "pad_idx": 0,
        "eos_idx": 3,
        "save_dis": 4
    },
    "device": "cuda"
}