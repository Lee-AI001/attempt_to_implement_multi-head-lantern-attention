
# Attempt to Implement MLA (Multi-Head Lantern Attention)

Welcome!

This repository focus on implementing a new attention mechanism called **Multi-Head Lantern Attention (MLA)**. This was originally part of my Extended Essay research project for my high school peoject — a bold and ambitious move, but I did not regret it! 

## 📁 Structure

- `MHA/` — Implementation of standard **Multi-Head Attention**
- `MLA/` — My custom attempt to build **Multi-Head Lantern Attention**

## 📉 Observations

While the training loss for MLA decreases rapidly, the output text becomes chaotic, unlike the MHA baseline. I’ve included training logs, generation samples, and comparison results inside each folder.

## 🙏 Call for Help

I'm not a Python expert yet, so if you spot something off in the MLA implementation — especially in how attention is calculated — I’d be grateful for your feedback or suggestions!

## 📬 Contact

Feel free to open an issue or pull request. Or reach out if you're curious, want to collaborate, or just love experimental AI ideas!

Thanks for stopping by!  
– **Lee Ling**


