
# Attempt to Implement MLA (Multi-Head Lantern Attention)

Welcome!

This repository focus on implementing a new attention mechanism called **Multi-Head Lantern Attention (MLA)**. This was originally part of my Extended Essay research project for my high school peoject â€” a bold and ambitious move, but I did not regret it! 

## ğŸ“ Structure

- `MHA/` â€” Implementation of standard **Multi-Head Attention**
- `MLA/` â€” My custom attempt to build **Multi-Head Lantern Attention**

## ğŸ“‰ Observations

While the training loss for MLA decreases rapidly, the output text becomes chaotic, unlike the MHA baseline. Iâ€™ve included training logs, generation samples, and comparison results inside each folder.

## ğŸ™ Call for Help

I'm not a Python expert yet, so if you spot something off in the MLA implementation â€” especially in how attention is calculated â€” Iâ€™d be grateful for your feedback or suggestions!

## ğŸ“¬ Contact

Feel free to open an issue or pull request. Or reach out if you're curious, want to collaborate, or just love experimental AI ideas!

Thanks for stopping by!  
â€“ **Lee Ling**


